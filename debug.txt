


 GLOBAL VARIABLES 



{'__name__': '__main__', '__doc__': '\nTranslate pre-processed data with a trained model.\n', '__package__': None, '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7f77c12a20>, '__spec__': None, '__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__file__': 'dHAT_generate.py', '__cached__': None, 'torch': <module 'torch' from '/home/hmrp1r17/.local/lib/python3.6/site-packages/torch/__init__.py'>, 'bleu': <module 'fairseq.bleu' from '/home/hmrp1r17/p3_final/d-HAT/fairseq/bleu.py'>, 'checkpoint_utils': <module 'fairseq.checkpoint_utils' from '/home/hmrp1r17/p3_final/d-HAT/fairseq/checkpoint_utils.py'>, 'options': <module 'fairseq.options' from '/home/hmrp1r17/p3_final/d-HAT/fairseq/options.py'>, 'progress_bar': <module 'fairseq.progress_bar' from '/home/hmrp1r17/p3_final/d-HAT/fairseq/progress_bar.py'>, 'tasks': <module 'fairseq.tasks' from '/home/hmrp1r17/p3_final/d-HAT/fairseq/tasks/__init__.py'>, 'utils': <module 'fairseq.utils' from '/home/hmrp1r17/p3_final/d-HAT/fairseq/utils.py'>, 'StopwatchMeter': <class 'fairseq.meters.StopwatchMeter'>, 'TimeMeter': <class 'fairseq.meters.TimeMeter'>, 'sys': <module 'sys' (built-in)>, 'pdb': <module 'pdb' from '/usr/lib/python3.6/pdb.py'>, 'np': <module 'numpy' from '/home/hmrp1r17/.local/lib/python3.6/site-packages/numpy/__init__.py'>, 'time': <built-in function time>, 'model2000args': {'encoder': {'encoder_embed_dim': 512, 'encoder_layer_num': 6, 'encoder_ffn_embed_dim': [3072, 3072, 3072, 2048, 3072, 2048], 'encoder_self_attention_heads': [8, 8, 4, 4, 8, 4]}, 'decoder': {'decoder_embed_dim': 512, 'decoder_layer_num': 6, 'decoder_ffn_embed_dim': [3072, 3072, 3072, 3072, 3072, 3072], 'decoder_self_attention_heads': [8, 8, 4, 8, 4, 4], 'decoder_ende_attention_heads': [8, 8, 8, 4, 8, 8], 'decoder_arbitrary_ende_attn': [-1, 1, 1, 1, 1, -1]}}, 'model1250args': {'encoder': {'encoder_embed_dim': 512, 'encoder_layer_num': 6, 'encoder_ffn_embed_dim': [3072, 3072, 3072, 2048, 3072, 3072], 'encoder_self_attention_heads': [8, 8, 8, 4, 8, 4]}, 'decoder': {'decoder_embed_dim': 512, 'decoder_layer_num': 5, 'decoder_ffn_embed_dim': [3072, 3072, 3072, 3072, 3072], 'decoder_self_attention_heads': [4, 8, 8, 4, 4], 'decoder_ende_attention_heads': [8, 8, 8, 8, 8], 'decoder_arbitrary_ende_attn': [-1, 1, 1, 1, -1]}}, 'model1000args': {'encoder': {'encoder_embed_dim': 512, 'encoder_layer_num': 6, 'encoder_ffn_embed_dim': [3072, 3072, 3072, 2048, 3072, 3072], 'encoder_self_attention_heads': [8, 8, 8, 4, 8, 4]}, 'decoder': {'decoder_embed_dim': 512, 'decoder_layer_num': 4, 'decoder_ffn_embed_dim': [3072, 3072, 3072, 3072], 'decoder_self_attention_heads': [8, 8, 8, 4], 'decoder_ende_attention_heads': [8, 8, 8, 8], 'decoder_arbitrary_ende_attn': [1, 1, 1, -1]}}, 'model900args': {'encoder': {'encoder_embed_dim': 512, 'encoder_layer_num': 6, 'encoder_ffn_embed_dim': [3072, 3072, 3072, 2048, 3072, 3072], 'encoder_self_attention_heads': [8, 8, 4, 8, 8, 8]}, 'decoder': {'decoder_embed_dim': 512, 'decoder_layer_num': 3, 'decoder_ffn_embed_dim': [3072, 3072, 3072], 'decoder_self_attention_heads': [8, 8, 8], 'decoder_ende_attention_heads': [8, 8, 8], 'decoder_arbitrary_ende_attn': [1, 1, 1]}}, 'model700args': {'encoder': {'encoder_embed_dim': 512, 'encoder_layer_num': 6, 'encoder_ffn_embed_dim': [3072, 3072, 3072, 2048, 3072, 3072], 'encoder_self_attention_heads': [8, 8, 8, 8, 8, 4]}, 'decoder': {'decoder_embed_dim': 512, 'decoder_layer_num': 2, 'decoder_ffn_embed_dim': [3072, 3072], 'decoder_self_attention_heads': [8, 8], 'decoder_ende_attention_heads': [8, 8], 'decoder_arbitrary_ende_attn': [1, 1]}}, 'model350args': {'encoder': {'encoder_embed_dim': 512, 'encoder_layer_num': 6, 'encoder_ffn_embed_dim': [2048, 3072, 3072, 3072, 3072, 2048], 'encoder_self_attention_heads': [8, 8, 4, 8, 8, 8]}, 'decoder': {'decoder_embed_dim': 512, 'decoder_layer_num': 1, 'decoder_ffn_embed_dim': [3072], 'decoder_self_attention_heads': [8], 'decoder_ende_attention_heads': [8], 'decoder_arbitrary_ende_attn': [-1]}}, 'modelconfigs': {'350': {'encoder': {'encoder_embed_dim': 512, 'encoder_layer_num': 6, 'encoder_ffn_embed_dim': [2048, 3072, 3072, 3072, 3072, 2048], 'encoder_self_attention_heads': [8, 8, 4, 8, 8, 8]}, 'decoder': {'decoder_embed_dim': 512, 'decoder_layer_num': 1, 'decoder_ffn_embed_dim': [3072], 'decoder_self_attention_heads': [8], 'decoder_ende_attention_heads': [8], 'decoder_arbitrary_ende_attn': [-1]}}, '700': {'encoder': {'encoder_embed_dim': 512, 'encoder_layer_num': 6, 'encoder_ffn_embed_dim': [3072, 3072, 3072, 2048, 3072, 3072], 'encoder_self_attention_heads': [8, 8, 8, 8, 8, 4]}, 'decoder': {'decoder_embed_dim': 512, 'decoder_layer_num': 2, 'decoder_ffn_embed_dim': [3072, 3072], 'decoder_self_attention_heads': [8, 8], 'decoder_ende_attention_heads': [8, 8], 'decoder_arbitrary_ende_attn': [1, 1]}}, '900': {'encoder': {'encoder_embed_dim': 512, 'encoder_layer_num': 6, 'encoder_ffn_embed_dim': [3072, 3072, 3072, 2048, 3072, 3072], 'encoder_self_attention_heads': [8, 8, 4, 8, 8, 8]}, 'decoder': {'decoder_embed_dim': 512, 'decoder_layer_num': 3, 'decoder_ffn_embed_dim': [3072, 3072, 3072], 'decoder_self_attention_heads': [8, 8, 8], 'decoder_ende_attention_heads': [8, 8, 8], 'decoder_arbitrary_ende_attn': [1, 1, 1]}}, '1000': {'encoder': {'encoder_embed_dim': 512, 'encoder_layer_num': 6, 'encoder_ffn_embed_dim': [3072, 3072, 3072, 2048, 3072, 3072], 'encoder_self_attention_heads': [8, 8, 8, 4, 8, 4]}, 'decoder': {'decoder_embed_dim': 512, 'decoder_layer_num': 4, 'decoder_ffn_embed_dim': [3072, 3072, 3072, 3072], 'decoder_self_attention_heads': [8, 8, 8, 4], 'decoder_ende_attention_heads': [8, 8, 8, 8], 'decoder_arbitrary_ende_attn': [1, 1, 1, -1]}}, '1250': {'encoder': {'encoder_embed_dim': 512, 'encoder_layer_num': 6, 'encoder_ffn_embed_dim': [3072, 3072, 3072, 2048, 3072, 3072], 'encoder_self_attention_heads': [8, 8, 8, 4, 8, 4]}, 'decoder': {'decoder_embed_dim': 512, 'decoder_layer_num': 5, 'decoder_ffn_embed_dim': [3072, 3072, 3072, 3072, 3072], 'decoder_self_attention_heads': [4, 8, 8, 4, 4], 'decoder_ende_attention_heads': [8, 8, 8, 8, 8], 'decoder_arbitrary_ende_attn': [-1, 1, 1, 1, -1]}}, '2000': {'encoder': {'encoder_embed_dim': 512, 'encoder_layer_num': 6, 'encoder_ffn_embed_dim': [3072, 3072, 3072, 2048, 3072, 2048], 'encoder_self_attention_heads': [8, 8, 4, 4, 8, 4]}, 'decoder': {'decoder_embed_dim': 512, 'decoder_layer_num': 6, 'decoder_ffn_embed_dim': [3072, 3072, 3072, 3072, 3072, 3072], 'decoder_self_attention_heads': [8, 8, 4, 8, 4, 4], 'decoder_ende_attention_heads': [8, 8, 8, 4, 8, 8], 'decoder_arbitrary_ende_attn': [-1, 1, 1, 1, 1, -1]}}}, 'modelargs': {}, 'outFile': None, 'main': <function main at 0x7f77c10048>, 'cli_main': <function cli_main at 0x7f2d700048>}



 LOCAL VARIABLES 



{'task': <fairseq.tasks.translation.TranslationTask object at 0x7f2868ec88>, 'use_cuda': True, 'build_start': 1619090497.14403, 'loop_count': 6, 'args': Namespace(beam=4, configs='configs/wmt14.en-de/subtransformer/wmt14ende_gpu_jetson_search0@1000ms.yml', cpu=False, criterion='cross_entropy', data='data/binary/wmt16_en_de', dataset_impl=None, decoder_arbitrary_ende_attn_all_subtransformer=[1, 1, 1, -1], decoder_embed_dim_subtransformer=512, decoder_ende_attention_heads_all_subtransformer=[8, 8, 8, 8], decoder_ffn_embed_dim_all_subtransformer=[3072, 3072, 3072, 3072], decoder_layer_num_subtransformer=4, decoder_self_attention_heads_all_subtransformer=[8, 8, 8, 4], diverse_beam_groups=-1, diverse_beam_strength=0.5, encoder_embed_dim_subtransformer=512, encoder_ffn_embed_dim_all_subtransformer=[3072, 3072, 3072, 2048, 3072, 3072], encoder_layer_num_subtransformer=6, encoder_self_attention_heads_all_subtransformer=[8, 8, 8, 4, 8, 4], force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', lat_config='900', lazy_load=False, left_pad_source=False, left_pad_target=False, lenpen=0.6, log_format='tqdm', log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=10, optimizer='nag', path='./checkpoints/wmt14.en-de/supertransformer/HAT_wmt14ende_const_super_space1.pt', pdb=False, prefix_size=0, print_alignment=False, profile_latency=False, quiet=False, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang='en', target_lang='de', task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir=None, warmup_updates=0, weight_decay=0.0), 'dFile': <_io.TextIOWrapper name='debug.txt' mode='w' encoding='UTF-8'>, 'prefix_tokens': None, 'sample': {'id': tensor([1, 4, 2, 3, 0], device='cuda:0'), 'nsentences': 5, 'ntokens': 107, 'net_input': {'src_tokens': tensor([[  747,    32,    55,   333,   892,  4350,  8796,    43,   671, 16116,
             4,     6,   146,   941,  5422, 21286, 13741,     7, 24016, 22071,
          2857,     7,  9850,  3830,  2931,  8033,    37,   414,   478,     8,
             6,  2043,  8878,  8228,  3950, 13741,     5,     2],
        [   47,   758,     6,   150,     4,     6,  8878,  8228,  3950, 13741,
           202,  5369,   270,    39,    65,    14,  3072,  5554,     4,    47,
         10295, 20709,   802,  6118,     5,     2,     1,     1,     1,     1,
             1,     1,     1,     1,     1,     1,     1,     1],
        [ 6785,  5526,     8, 13741,    83,   972,    12,   108,   871,    43,
         12980,  4515,    62,   295,    14,  9274,   773,  7102,    89,     2,
             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
             1,     1,     1,     1,     1,     1,     1,     1],
        [25322,     4,  9850,  4762,   100, 26779,  4697,    14,   632,  2429,
            12,    39,   593,     5,     2,     1,     1,     1,     1,     1,
             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
             1,     1,     1,     1,     1,     1,     1,     1],
        [ 9850,  3830,    43, 24249,  8225,  1312,    18, 30489,   966,     2,
             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
             1,     1,     1,     1,     1,     1,     1,     1]],
       device='cuda:0'), 'src_lengths': tensor([38, 26, 20, 15, 10], device='cuda:0'), 'prev_output_tokens': tensor([[    2,    41,   626,   278,   892,  4581, 16229,  1003,    43,  1691,
         18235,    21,     7,  9850,  3830,    10,   384,   941,   513,  3952,
         25479,  2398,   330,   106, 24016, 22071,  2857,     7,  5462,  2364,
           816,    37,     7, 20563,  8956,    11, 12110, 21183,   316,  2398,
           330,     5],
        [    2,    47,    56, 21183,   316,  2398,   330,    21,  6153,  7376,
           816,     4,   503,   120,    19,  4213,  3049, 20111,    47,     4,
         30750, 24664, 20709,   802,  6898,     5,     1,     1,     1,     1,
             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
             1,     1],
        [    2,  9551,  6099,    83, 12701,  1853,  5320,    43,  7319,    75,
         19427,   219, 25169,   302,  6086,    89,     1,     1,     1,     1,
             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
             1,     1],
        [    2,   346,   456,    87,  9850,  1400,    71,  5020, 11645,  6898,
          1485, 12185,     5,     1,     1,     1,     1,     1,     1,     1,
             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
             1,     1],
        [    2,  9850,  3830,    43,  8549,   193,   850,    23, 30932,  1049,
             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
             1,     1]], device='cuda:0')}, 'target': tensor([[   41,   626,   278,   892,  4581, 16229,  1003,    43,  1691, 18235,
            21,     7,  9850,  3830,    10,   384,   941,   513,  3952, 25479,
          2398,   330,   106, 24016, 22071,  2857,     7,  5462,  2364,   816,
            37,     7, 20563,  8956,    11, 12110, 21183,   316,  2398,   330,
             5,     2],
        [   47,    56, 21183,   316,  2398,   330,    21,  6153,  7376,   816,
             4,   503,   120,    19,  4213,  3049, 20111,    47,     4, 30750,
         24664, 20709,   802,  6898,     5,     2,     1,     1,     1,     1,
             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
             1,     1],
        [ 9551,  6099,    83, 12701,  1853,  5320,    43,  7319,    75, 19427,
           219, 25169,   302,  6086,    89,     2,     1,     1,     1,     1,
             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
             1,     1],
        [  346,   456,    87,  9850,  1400,    71,  5020, 11645,  6898,  1485,
         12185,     5,     2,     1,     1,     1,     1,     1,     1,     1,
             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
             1,     1],
        [ 9850,  3830,    43,  8549,   193,   850,    23, 30932,  1049,     2,
             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
             1,     1]], device='cuda:0')}, 'wps_meter': <fairseq.meters.TimeMeter object at 0x7f2868e550>, 't': <fairseq.progress_bar.tqdm_progress_bar object at 0x7f2868ec18>, 'input_len_all': [], 'decoder_times_all': [], 'has_target': True, 'num_sentences': 0, 'generator': <fairseq.sequence_generator.SequenceGenerator object at 0x7f2868ee10>, 'gen_timer': <fairseq.meters.StopwatchMeter object at 0x7f2bed3ac8>, 'inference_start': 1619091091.8330595, 'lat_end': 1619091091.8330593, 'itr': <fairseq.data.iterators.CountingIterator object at 0x7f2868ed68>, 'align_dict': None, 'config': {'encoder': {'encoder_embed_dim': 512, 'encoder_layer_num': 6, 'encoder_ffn_embed_dim': [3072, 3072, 3072, 2048, 3072, 3072], 'encoder_self_attention_heads': [8, 8, 8, 4, 8, 4]}, 'decoder': {'decoder_embed_dim': 512, 'decoder_layer_num': 4, 'decoder_ffn_embed_dim': [3072, 3072, 3072, 3072], 'decoder_self_attention_heads': [8, 8, 8, 4], 'decoder_ende_attention_heads': [8, 8, 8, 8], 'decoder_arbitrary_ende_attn': [1, 1, 1, -1]}}, 'model': TransformerSuperModel(
  (encoder): TransformerEncoder(
    (embed_tokens): EmbeddingSuper(32768, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (fc1): LinearSuper(in_features=512, out_features=3072, bias=True)
        (fc2): LinearSuper(in_features=3072, out_features=512, bias=True)
        (final_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (fc1): LinearSuper(in_features=512, out_features=3072, bias=True)
        (fc2): LinearSuper(in_features=3072, out_features=512, bias=True)
        (final_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttentionSuper	num_heads:4	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (fc1): LinearSuper(in_features=512, out_features=3072, bias=True)
        (fc2): LinearSuper(in_features=3072, out_features=512, bias=True)
        (final_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (fc1): LinearSuper(in_features=512, out_features=3072, bias=True)
        (fc2): LinearSuper(in_features=3072, out_features=512, bias=True)
        (final_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (fc1): LinearSuper(in_features=512, out_features=3072, bias=True)
        (fc2): LinearSuper(in_features=3072, out_features=512, bias=True)
        (final_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (fc1): LinearSuper(in_features=512, out_features=3072, bias=True)
        (fc2): LinearSuper(in_features=3072, out_features=512, bias=True)
        (final_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): EmbeddingSuper(32768, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (fc1): LinearSuper(in_features=512, out_features=3072, bias=True)
        (fc2): LinearSuper(in_features=3072, out_features=512, bias=True)
        (final_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (fc1): LinearSuper(in_features=512, out_features=3072, bias=True)
        (fc2): LinearSuper(in_features=3072, out_features=512, bias=True)
        (final_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (fc1): LinearSuper(in_features=512, out_features=3072, bias=True)
        (fc2): LinearSuper(in_features=3072, out_features=512, bias=True)
        (final_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttentionSuper	num_heads:4	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (fc1): LinearSuper(in_features=512, out_features=3072, bias=True)
        (fc2): LinearSuper(in_features=3072, out_features=512, bias=True)
        (final_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttentionSuper	num_heads:4	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (fc1): LinearSuper(in_features=512, out_features=3072, bias=True)
        (fc2): LinearSuper(in_features=3072, out_features=512, bias=True)
        (final_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttentionSuper	num_heads:4	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (fc1): LinearSuper(in_features=512, out_features=3072, bias=True)
        (fc2): LinearSuper(in_features=3072, out_features=512, bias=True)
        (final_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
), 'tgt_dict': <fairseq.data.dictionary.Dictionary object at 0x7f2868ec50>, 'src_dict': <fairseq.data.dictionary.Dictionary object at 0x7f2868e978>, 'dFile2': <_io.TextIOWrapper name='debug_task.txt' mode='a' encoding='UTF-8'>, 'outFile': <_io.TextIOWrapper name='d-HAT_output.txt' mode='a' encoding='UTF-8'>, 'lat_start': 1619091091.3824446, 'input_lat': '900', 'build_end': 1619090512.9819386, '_model_args': Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformersuper_wmt_en_de', attention_dropout=0.1, beam=5, best_checkpoint_metric='loss', bucket_cap_mb=25, clip_norm=0.0, configs='configs/wmt14.en-de/supertransformer/space0.yml', cpu=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='data/binary/wmt16_en_de', dataset_impl=None, ddp_backend='no_c10d', decoder_arbitrary_ende_attn_all_subtransformer=None, decoder_arbitrary_ende_attn_choice=[-1, 1], decoder_attention_heads=8, decoder_embed_choice=[512], decoder_embed_dim=512, decoder_embed_dim_subtransformer=None, decoder_embed_path=None, decoder_ende_attention_heads_all_subtransformer=None, decoder_ende_attention_heads_choice=[8, 4], decoder_ffn_embed_dim=3072, decoder_ffn_embed_dim_all_subtransformer=None, decoder_ffn_embed_dim_choice=[3072, 2048], decoder_input_dim=512, decoder_layer_num_choice=[6, 5, 4, 3, 2, 1], decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, decoder_self_attention_heads_all_subtransformer=None, decoder_self_attention_heads_choice=[8, 4], device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:10328', distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=2, diverse_beam_groups=-1, diverse_beam_strength=0.5, dropout=0.3, encoder_attention_heads=8, encoder_embed_choice=[512], encoder_embed_dim=512, encoder_embed_dim_subtransformer=None, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_ffn_embed_dim_all_subtransformer=None, encoder_ffn_embed_dim_choice=[3072, 2048], encoder_layer_num_choice=[6], encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=False, encoder_self_attention_heads_all_subtransformer=None, encoder_self_attention_heads_choice=[8, 4], find_unused_parameters=False, fix_batches_to_gpus=False, fp16=True, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, get_attn=False, init_method='xavier', keep_interval_updates=-1, keep_last_epochs=20, label_smoothing=0.1, latcpu=False, latgpu=False, latiter=300, latsilent=False, lazy_load=False, left_pad_source=True, left_pad_target=False, lenpen=1, log_format='tqdm', log_interval=1000, lr=[1e-07], lr_period_updates=-1, lr_scheduler='cosine', lr_shrink=1.0, match_source_len=False, max_epoch=0, max_len_a=0, max_len_b=200, max_lr=0.001, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=40000, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, min_lr=-1, model_overrides='{}', nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_repeat_ngram_size=0, no_save=False, no_save_optimizer_state=False, no_token_positional_embeddings=False, num_workers=10, optimizer='adam', optimizer_overrides='{}', path=None, pdb=False, prefix_size=0, print_alignment=False, profile_flops=False, profile_latency=False, qkv_dim=512, quiet=False, raw_text=False, remove_bpe=None, replace_unk=None, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, save_dir='checkpoints/wmt14.en-de/supertransformer/space0', save_interval=2, save_interval_updates=0, score_reference=False, seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, source_lang='en', sub_configs=None, t_mult=1, target_lang='de', task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='checkpoints/wmt14.en-de/supertransformer/space0/tensorboard', threshold_loss_scale=None, train_subset='train', train_subtransformer=False, unkpen=0, unnormalized=False, update_freq=[64], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=10, validate_subtransformer=False, vocab_original_scaling=False, warmup_init_lr=1e-07, warmup_updates=10000, weight_decay=0.0), 'models': [TransformerSuperModel(
  (encoder): TransformerEncoder(
    (embed_tokens): EmbeddingSuper(32768, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (fc1): LinearSuper(in_features=512, out_features=3072, bias=True)
        (fc2): LinearSuper(in_features=3072, out_features=512, bias=True)
        (final_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (fc1): LinearSuper(in_features=512, out_features=3072, bias=True)
        (fc2): LinearSuper(in_features=3072, out_features=512, bias=True)
        (final_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttentionSuper	num_heads:4	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (fc1): LinearSuper(in_features=512, out_features=3072, bias=True)
        (fc2): LinearSuper(in_features=3072, out_features=512, bias=True)
        (final_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (fc1): LinearSuper(in_features=512, out_features=3072, bias=True)
        (fc2): LinearSuper(in_features=3072, out_features=512, bias=True)
        (final_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (fc1): LinearSuper(in_features=512, out_features=3072, bias=True)
        (fc2): LinearSuper(in_features=3072, out_features=512, bias=True)
        (final_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (fc1): LinearSuper(in_features=512, out_features=3072, bias=True)
        (fc2): LinearSuper(in_features=3072, out_features=512, bias=True)
        (final_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): EmbeddingSuper(32768, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (fc1): LinearSuper(in_features=512, out_features=3072, bias=True)
        (fc2): LinearSuper(in_features=3072, out_features=512, bias=True)
        (final_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (fc1): LinearSuper(in_features=512, out_features=3072, bias=True)
        (fc2): LinearSuper(in_features=3072, out_features=512, bias=True)
        (final_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (fc1): LinearSuper(in_features=512, out_features=3072, bias=True)
        (fc2): LinearSuper(in_features=3072, out_features=512, bias=True)
        (final_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttentionSuper	num_heads:4	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (fc1): LinearSuper(in_features=512, out_features=3072, bias=True)
        (fc2): LinearSuper(in_features=3072, out_features=512, bias=True)
        (final_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttentionSuper	num_heads:4	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (fc1): LinearSuper(in_features=512, out_features=3072, bias=True)
        (fc2): LinearSuper(in_features=3072, out_features=512, bias=True)
        (final_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttentionSuper	num_heads:4	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttentionSuper	num_heads:8	 qkv_dim:512
          (out_proj): LinearSuper(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
        (fc1): LinearSuper(in_features=512, out_features=3072, bias=True)
        (fc2): LinearSuper(in_features=3072, out_features=512, bias=True)
        (final_layer_norm): LayerNormSuper((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)], 'inference_end': 1619090647.7865746, 'alignment': None, 'hypo_str': 'Mehr Sicherheit für Fußgänger', 'hypo_tokens': tensor([ 6611,   850,    23, 32777,     2], dtype=torch.int32), 'hypo': {'tokens': tensor([ 6611,   850,    23, 30932,  1049,     2], device='cuda:0'), 'score': -1.2123277187347412, 'attention': None, 'alignment': None, 'positional_scores': tensor([-1.6441, -0.0914, -0.8716, -0.3721, -0.0253, -0.5478], device='cuda:0')}, 'j': 0, 'target_str': 'Gutach : Noch mehr Sicherheit für Fußgänger', 'src_str': 'Gutach : Increased safety for pedestrians', 'target_tokens': tensor([ 9850,  3830,    43,  8549,   193,   850,    23, 30932,  1049,     2],
       dtype=torch.int32), 'src_tokens': tensor([ 9850,  3830,    43, 24249,  8225,  1312,    18, 30489,   966,     2],
       device='cuda:0'), 'sample_id': 0, 'i': 4, 'num_generated_tokens': 90, 'decoder_times': 56, 'hypos': [[{'tokens': tensor([   72,    11,  9850,  1400,   365, 25479,  3369,  2049,    54,  2352,
           53,  9850,  1400,   365, 21183,    95,   145,   278,   432,  7357,
        18658,     5,     2], device='cuda:0'), 'score': -4.895914077758789, 'attention': None, 'alignment': None, 'positional_scores': tensor([-2.8224, -0.6913, -3.4136, -0.3453, -1.1072, -1.8675, -0.0895, -1.5791,
        -0.0701, -2.2025, -1.3796, -2.3950, -0.1525, -1.6964, -0.9731, -0.2428,
        -1.4638, -2.3607, -2.1597, -1.6064, -1.7295, -1.5330, -0.2461],
       device='cuda:0')}, {'tokens': tensor([   72,    11,  9850,  1400,   365, 25479,  3369,  2049,    54,  2352,
           53,  9850,  1400,   365, 21183,    95,   145,    38,   384,  7357,
         4440,   245, 11056,   513,  6517,     2], device='cuda:0'), 'score': -5.106493949890137, 'attention': None, 'alignment': None, 'positional_scores': tensor([-2.8224, -0.6913, -3.4136, -0.3453, -1.1072, -1.8675, -0.0895, -1.5791,
        -0.0701, -2.2025, -1.3796, -2.3950, -0.1525, -1.6964, -0.9731, -0.2428,
        -1.4638, -2.3935, -0.9368, -1.9785, -2.1473, -0.4923, -2.6509, -1.8577,
        -0.1502, -0.9679], device='cuda:0')}, {'tokens': tensor([   72,    11,  9850,  1400,   365, 25479,  3369,  2049,    54,  2352,
           53,  9850,  1400,   365, 21183,    95,   145,    38,   384,  7357,
         4440,   245, 17255,  4034,     5,     2], device='cuda:0'), 'score': -5.174775123596191, 'attention': None, 'alignment': None, 'positional_scores': tensor([-2.8224, -0.6913, -3.4136, -0.3453, -1.1072, -1.8675, -0.0895, -1.5791,
        -0.0701, -2.2025, -1.3796, -2.3950, -0.1525, -1.6964, -0.9731, -0.2428,
        -1.4638, -2.3935, -0.9368, -1.9785, -2.1473, -0.4923, -3.0576, -0.2257,
        -2.5305, -0.2951], device='cuda:0')}, {'tokens': tensor([   72,    11,  9850,  1400,   365, 25479,  3369,  2049,    54,  2352,
           53,  9850,  1400,   365, 21183,    95,   145,    38,   384,  7357,
         4440,   245,   300,  2398,   188,  3434,   513,  6517,     2],
       device='cuda:0'), 'score': -5.374154090881348, 'attention': None, 'alignment': None, 'positional_scores': tensor([-2.8224, -0.6913, -3.4136, -0.3453, -1.1072, -1.8675, -0.0895, -1.5791,
        -0.0701, -2.2025, -1.3796, -2.3950, -0.1525, -1.6964, -0.9731, -0.2428,
        -1.4638, -2.3935, -0.9368, -1.9785, -2.1473, -0.4923, -2.9829, -1.0416,
        -0.5963, -2.8552, -1.3906, -0.1217, -1.0991], device='cuda:0')}], [{'tokens': tensor([   47, 25186,   148,     7,    11, 13949,    38,  3299,   300,  2398,
          188,  7376,     5,     2], device='cuda:0'), 'score': -3.116631269454956, 'attention': None, 'alignment': None, 'positional_scores': tensor([-1.1064, -2.3245, -0.8166, -2.5670, -0.8841, -0.9055, -1.1402, -2.1269,
        -1.1783, -0.5642, -0.0536, -0.6068, -0.8107, -0.0984], device='cuda:0')}, {'tokens': tensor([   47, 25186,   148,     7,    11, 13949,    38, 17033,   330,  7376,
            5,     2], device='cuda:0'), 'score': -3.172748565673828, 'attention': None, 'alignment': None, 'positional_scores': tensor([-1.1064, -2.3245, -0.8166, -2.5670, -0.8841, -0.9055, -1.1402, -2.5298,
        -0.3698, -0.5740, -0.7774, -0.0958], device='cuda:0')}, {'tokens': tensor([   47, 25186,   148,     7,    11, 13949,    38,  6983,  9133,   300,
         2398,   188,  7376,     5,     2], device='cuda:0'), 'score': -3.3752849102020264, 'attention': None, 'alignment': None, 'positional_scores': tensor([-1.1064, -2.3245, -0.8166, -2.5670, -0.8841, -0.9055, -1.1402, -2.1656,
        -1.8712, -1.3144, -0.3818, -0.0606, -0.5631, -0.9384, -0.0988],
       device='cuda:0')}, {'tokens': tensor([   47, 25186,   148,     7,    11, 13949,    38,  3299,   300,  2398,
          188,  7376,     4,   503,    10, 17033,   330,  7376,   148,     5,
            2], device='cuda:0'), 'score': -3.5746164321899414, 'attention': None, 'alignment': None, 'positional_scores': tensor([-1.1064, -2.3245, -0.8166, -2.5670, -0.8841, -0.9055, -1.1402, -2.1269,
        -1.1783, -0.5642, -0.0536, -0.6068, -0.9285, -0.5401, -0.9553, -3.0600,
        -0.2302, -1.0887, -0.8790, -0.1620, -0.0929], device='cuda:0')}], [{'tokens': tensor([ 9551, 11959,  3238,   335,   199, 25074,   180,  1383,    43,   487,
         2444,  3219,  2960,   197, 11832,    49,    75,    49,  4505,   380,
        10551,    20,   111,   301,   122,    49,  4505,   380, 10551,   216,
        29631,  9093,   535,    89,     2], device='cuda:0'), 'score': -5.816837787628174, 'attention': None, 'alignment': None, 'positional_scores': tensor([-1.1168, -3.1452, -0.6026, -0.1744, -0.0996, -0.5005, -0.0674, -1.6420,
        -2.7672, -2.3424, -1.6299, -1.4590, -1.6582, -2.7901, -0.0327, -2.1892,
        -1.2468, -1.3528, -0.4859, -2.9860, -0.0882, -2.3266, -1.1832, -0.6661,
        -2.3160, -1.4898, -1.6263, -3.1385, -0.2322, -1.3648, -1.1182, -0.2912,
        -2.3764, -2.5123, -0.0867], device='cuda:0')}, {'tokens': tensor([ 9551, 11959,  3238,   335,   199, 25074,   180,  1383,    43,   487,
         2444,  3219,  2960,   197, 11832,    49,    75,    49,  4505,   380,
        10551,    20,   111,   301,   122,    49,  4505,   380, 10551,   216,
        29631,  9093,   535,    43,    49,  4505,   380, 10551,   216, 29631,
         9093,     5,     2], device='cuda:0'), 'score': -6.344104290008545, 'attention': None, 'alignment': None, 'positional_scores': tensor([-1.1168, -3.1452, -0.6026, -0.1744, -0.0996, -0.5005, -0.0674, -1.6420,
        -2.7672, -2.3424, -1.6299, -1.4590, -1.6582, -2.7901, -0.0327, -2.1892,
        -1.2468, -1.3528, -0.4859, -2.9860, -0.0882, -2.3266, -1.1832, -0.6661,
        -2.3160, -1.4898, -1.6263, -3.1385, -0.2322, -1.3648, -1.1182, -0.2912,
        -2.3764, -0.8014, -2.2523, -1.7166, -2.9138, -0.2360, -1.3467, -1.5319,
        -0.1686, -2.9509, -0.1725], device='cuda:0')}, {'tokens': tensor([ 9551, 11959,  3238,   335,   199, 25074,   180,  1383,    43,   487,
         2444,  3219,  2960,   197, 11832,    49,    75,    49,  4505,   380,
        10551,    20,   111,   301,   122,    49,  4505,   380, 10551,   216,
        29631,  9093,   535,    43,   487,  4505,   380, 10551,   216, 29631,
         9093,     5,     2], device='cuda:0'), 'score': -6.349996566772461, 'attention': None, 'alignment': None, 'positional_scores': tensor([-1.1168, -3.1452, -0.6026, -0.1744, -0.0996, -0.5005, -0.0674, -1.6420,
        -2.7672, -2.3424, -1.6299, -1.4590, -1.6582, -2.7901, -0.0327, -2.1892,
        -1.2468, -1.3528, -0.4859, -2.9860, -0.0882, -2.3266, -1.1832, -0.6661,
        -2.3160, -1.4898, -1.6263, -3.1385, -0.2322, -1.3648, -1.1182, -0.2912,
        -2.3764, -0.8014, -2.1696, -1.7069, -2.9086, -0.2465, -1.3388, -1.5834,
        -0.1616, -3.0579, -0.1724], device='cuda:0')}, {'tokens': tensor([ 9551, 11959,  3238,   335,   199, 25074,   180,  1383,    43,   487,
         2444,  3219,  2960,   197, 11832,    49,    75,    49,  4505,   380,
        10551,    20,   111,   301,   122,    49,  4505,   380, 10551,   216,
        29631,  9093,   535,    43,    49,  4505,   380, 10551,   216, 29631,
         9093,     4,    83, 17154,   800,     4,    75,    49,  4505,   380,
        10551,   216, 29631,  9093,     5,     2], device='cuda:0'), 'score': -7.1983160972595215, 'attention': None, 'alignment': None, 'positional_scores': tensor([-1.1168, -3.1452, -0.6026, -0.1744, -0.0996, -0.5005, -0.0674, -1.6420,
        -2.7672, -2.3424, -1.6299, -1.4590, -1.6582, -2.7901, -0.0327, -2.1892,
        -1.2468, -1.3528, -0.4859, -2.9860, -0.0882, -2.3266, -1.1832, -0.6661,
        -2.3160, -1.4898, -1.6263, -3.1385, -0.2322, -1.3648, -1.1182, -0.2912,
        -2.3764, -0.8014, -2.2523, -1.7166, -2.9138, -0.2360, -1.3467, -1.5319,
        -0.1686, -2.4633, -2.4541, -0.7481, -0.9874, -2.4314, -2.1973, -1.4938,
        -1.3095, -2.9862, -0.1721, -1.2920, -1.2693, -0.1452, -2.9553, -0.1857],
       device='cuda:0')}], [{'tokens': tensor([25186,    87,    10,  9850,  1400,   365,    38,  4737,  2044,  1690,
            5,     2], device='cuda:0'), 'score': -1.6650313138961792, 'attention': None, 'alignment': None, 'positional_scores': tensor([-1.1739, -1.3395, -0.7610, -0.1776, -0.3121, -0.6376, -0.9259, -0.7115,
        -0.7491, -0.3711, -0.1436, -0.0919], device='cuda:0')}, {'tokens': tensor([25186,    87,    10,  9850,  1400,   687, 26583,     7,    38,  4737,
         2044,  1690,     5,     2], device='cuda:0'), 'score': -1.825549840927124, 'attention': None, 'alignment': None, 'positional_scores': tensor([-1.1739, -1.3395, -0.7610, -0.1776, -0.3121, -0.9821, -1.3200, -0.1644,
        -0.4949, -0.6815, -0.8199, -0.4157, -0.1594, -0.0914], device='cuda:0')}, {'tokens': tensor([25186,  2073,    10,  9850,  1400,   687, 26583,     7,    38,  4737,
         2044,     5,     2], device='cuda:0'), 'score': -2.112133502960205, 'attention': None, 'alignment': None, 'positional_scores': tensor([-1.1739, -0.8582, -0.8716, -0.2250, -0.3190, -0.9504, -1.6021, -0.1856,
        -0.3804, -0.6943, -0.5326, -1.9561, -0.0928], device='cuda:0')}, {'tokens': tensor([25186,  2073,    10,  9850,  1400,   687, 26583,     7,    38,  4737,
         2044,    33,    10,  9850,  1400,   365,  5020, 11645,     5,     2],
       device='cuda:0'), 'score': -2.35756254196167, 'attention': None, 'alignment': None, 'positional_scores': tensor([-1.1739, -0.8582, -0.8716, -0.2250, -0.3190, -0.9504, -1.6021, -0.1856,
        -0.3804, -0.6943, -0.5326, -1.0748, -1.2724, -0.2705, -0.4209, -0.4713,
        -0.8224, -1.1828, -0.8266, -0.0911], device='cuda:0')}], [{'tokens': tensor([ 6611,   850,    23, 30932,  1049,     2], device='cuda:0'), 'score': -1.2123277187347412, 'attention': None, 'alignment': None, 'positional_scores': tensor([-1.6441, -0.0914, -0.8716, -0.3721, -0.0253, -0.5478], device='cuda:0')}, {'tokens': tensor([ 6611,   850,    23, 30932,  1049,    43,     2], device='cuda:0'), 'score': -1.7196978330612183, 'attention': None, 'alignment': None, 'positional_scores': tensor([-1.6441, -0.0914, -0.8716, -0.3721, -0.0253, -1.8268, -0.6960],
       device='cuda:0')}, {'tokens': tensor([ 6611,   850,    20, 30932,  2196,     2], device='cuda:0'), 'score': -1.8752049207687378, 'attention': None, 'alignment': None, 'positional_scores': tensor([-1.6441, -0.0914, -2.1245, -1.2926, -0.0282, -0.3139], device='cuda:0')}, {'tokens': tensor([ 6611,   850,    11, 30932,  1049,     2], device='cuda:0'), 'score': -1.909861445426941, 'attention': None, 'alignment': None, 'positional_scores': tensor([-1.6441, -0.0914, -2.2564, -1.0901, -0.0163, -0.4979], device='cuda:0')}]]}




